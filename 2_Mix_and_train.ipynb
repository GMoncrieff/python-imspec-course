{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b687d4-d0c7-4686-a970-cb32c095e4ea",
   "metadata": {},
   "source": [
    "# Working with multidimensional spatial data in Python\n",
    "# lesson 2.1 - Processing imaging spectroscopy data at scale\n",
    "_Glenn Moncrieff_  \n",
    "[github.com/GMoncrieff](github.com/GMoncrieff)  \n",
    "[@glennwithtwons](https://twitter.com/glennwithtwons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51815a79-d8f0-4aa8-b7ad-8c421cce8224",
   "metadata": {},
   "source": [
    "Now that we are familiar with the main packages and concepts related to scalable processing of geospatial data in python we will start analysing some real imaging spectroscopy data. We will train a model that will be used to unmix the endmembers in a imaging spectroscopy data cube. Our goal is to map the distribution of invasive alien pine tress in the Cape florsitic region of South Africa using data from [The Earth Surface Mineral Dust Source Investigation (EMIT)](https://earth.jpl.nasa.gov/emit/).  \n",
    "  \n",
    "  \n",
    "The unmxing approach we will use is regression-based unmixing. This involves creating synthetic mixtures from pure endmembers with known mixing proportions. We then used these synthetic mixutres to train machine learning models. See [Okunjeni et al 2013 RSE](https://www.sciencedirect.com/science/article/abs/pii/S0034425713002009) for more background.    \n",
    "  \n",
    "  \n",
    "In this first notebook we access the EMIT data, create the synthetic mixtures, then train and save our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687e606-e482-4ba7-bd61-ebfe4b57048f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d11e01-4c1e-41d3-aea3-cfd55c2db496",
   "metadata": {},
   "source": [
    "### Authenticate with NASA Earthdata portal\n",
    "Earthaccess is a python library to search, download or stream NASA Earth science data. You will also need an account on NASA's Earthdata data portal\n",
    "https://search.earthdata.nasa.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96564acb-6375-4e7f-a9a0-69a318d5c28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "auth = earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9aaf4-d5e7-4094-82da-14199872bbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The old way\n",
    "#from utils.s3_access import write_creds\n",
    "#write_creds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf12a5-3e3d-43d1-961e-a5466b969b87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca5dd8-5464-4d46-9fb5-195737aca280",
   "metadata": {},
   "source": [
    "> a number of utility functions that I have written/copied are saved as `.py` files in the `utils` folder. We load these using `from utils.filename import functionname`. You can browse the files to understand the code, but I have moved the functions out of this notebook to reduce the cognitive load and keep the code neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25af780-a4c6-450d-acb5-01b58719d42a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import json\n",
    "import geopandas as gpd\n",
    "\n",
    "#ml modules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#ml models\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#our modules\n",
    "from utils.emit_tools import emit_xarray, quality_mask, gamma_adjust\n",
    "from utils.synthmix import extract_points, mix_fast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150873cb-57e0-4eaf-a530-8be844676b7e",
   "metadata": {},
   "source": [
    "## 3. Accessing and finding files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec817d-0213-47d0-8aaa-a114d0c47cb3",
   "metadata": {},
   "source": [
    "### Accessing data on AWS S3\n",
    "\n",
    "The emit files are stored on Amazon S3 object storage. We will access these directly without needing to download them as we can stream s3 files. Also, because we are using xarray with dask and our data is stored in a chunked file format `netcdf`, we only need to stream the chunks we are currently working on and not the entire file at once.\n",
    "  \n",
    "The best way to access the data is on a virtual machine located in `us-west-2`. This is where the data is located and this mean that will get the fastest reading and writing of data possible, and all read/writes will be free. If your machine is not in `us-west-2` or you are on a local machine, I do not recommend using S3, as it might be slower and there will be costs involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6d101-f47d-4ec9-b708-675953cc9aac",
   "metadata": {},
   "source": [
    "### Finding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae77d15-66c9-4cdf-8723-b334d06c8e01",
   "metadata": {},
   "source": [
    "earthaccess has really convineint methods for searching and loading nasa data over S3 using `earthaccess.search_data()`. Here is an example:\n",
    "```\n",
    "results = earthaccess.search_data(\n",
    "    short_name='EMITL2ARFL',\n",
    "    point=(19.03711,-33.94747),\n",
    "    temporal=('2023-01-18','2023-01-24'),\n",
    "    count=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085bf23-23bf-4596-b2fb-aaff0dbe0720",
   "metadata": {},
   "source": [
    "you can then load the results into an xarray\n",
    "```\n",
    "files = earthaccess.open(results)\n",
    "ds = xr.open_mfdataset(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd933c89-bc9b-42f2-9afc-2062ac963d39",
   "metadata": {},
   "source": [
    "if you do not want to stream the data from S3 you can download the data directly using:\n",
    "```\n",
    "files = earthaccess.download(results, \"./local_folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6782c7-cee4-405b-88af-e9d2dabb4ac9",
   "metadata": {},
   "source": [
    "for more info on using earthaccess see the [docs](https://nsidc.github.io/earthaccess/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f649e-a076-4f7b-a12d-6f1b228dbb16",
   "metadata": {},
   "source": [
    "I have already saved the links to the EMIT scenes we will analyses on S3 (you can find scenes interactively on [the earthdata serarch page](https://search.earthdata.nasa.gov/search) , so we will not use the `earthacess` package to find data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e1c51-38fb-4e9e-b194-20b6f08be24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#these are our files - the s3 links\n",
    "f_url = ['s3://lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230119T114247_2301907_005/EMIT_L2A_RFL_001_20230119T114247_2301907_005.nc',\n",
    "         's3://lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230123T100615_2302306_006/EMIT_L2A_RFL_001_20230123T100615_2302306_006.nc']\n",
    "f_mask_url = ['s3://lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230119T114247_2301907_005/EMIT_L2A_MASK_001_20230119T114247_2301907_005.nc',\n",
    "            's3://lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230123T100615_2302306_006/EMIT_L2A_MASK_001_20230123T100615_2302306_006.nc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dceb8a0-1713-4a5a-8e6f-f6863a7769e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Open the datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354c84c-6bed-4a99-bfbf-9a0d3ebb8303",
   "metadata": {
    "tags": []
   },
   "source": [
    "### If you are in us-west-2: Authenticate with s3 and stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4344b-9f09-454b-a9f2-044d5157ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load s3 credentials\n",
    "from utils.s3_access import get_temp_creds\n",
    "temp_creds_req = get_temp_creds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf82d7-2ae5-4309-a2ae-d09d4057b78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass Authentication to s3fs\n",
    "fs_s3 = s3fs.S3FileSystem(anon=False, \n",
    "                          key=temp_creds_req['accessKeyId'], \n",
    "                          secret=temp_creds_req['secretAccessKey'], \n",
    "                          token=temp_creds_req['sessionToken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69862441-0b66-4971-9807-c46d9e600c07",
   "metadata": {},
   "source": [
    "#### Link to the s3 file and have a quick look with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aa285-d79d-4205-a157-b7a31878bc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open s3 url\n",
    "fp = fs_s3.open(f_url[0], mode='rb')\n",
    "fp_mask = fs_s3.open(f_mask_url[0], mode='rb')\n",
    "\n",
    "# Open dataset with xarray\n",
    "ds = xr.open_dataset(fp,engine='h5netcdf',chunks='auto')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678b261-d875-41a0-9a77-ff52417c7174",
   "metadata": {
    "tags": []
   },
   "source": [
    "### If you are not in us-west-2: Download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab329b-4d21-4b62-b930-0a7d71c507af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the files we want\n",
    "results = earthaccess.search_data(\n",
    "    short_name='EMITL2ARFL',\n",
    "    point=(19.03711,-33.94747),\n",
    "    temporal=('2023-01-18','2023-01-24'),\n",
    "    count=1\n",
    ")\n",
    "#download them\n",
    "files = earthaccess.download(results, \"data/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48f5aa-244d-4095-98b0-afff324cb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset with xarray\n",
    "fp = 'data/downloads/EMIT_L2A_RFL_001_20230119T114247_2301907_005.nc'\n",
    "fp_mask = 'data/downloads/EMIT_L2A_MASK_001_20230119T114247_2301907_005.nc'\n",
    "\n",
    "ds = xr.open_dataset(fp,engine='h5netcdf',chunks='auto')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2589c-343c-4575-9a01-ad02c1f2d229",
   "metadata": {},
   "source": [
    "### Load the mask\n",
    "The EMIT data is composed of 3 files, the actual data, a mask that tells us about various quality issues with the data, and the reflectance uncertainties (we will ignore the uncertainty for now). We can selecting which quality filter (mask) to use. Here we used the aggregate filter - the sum of all filters to be very strict about what data we include. The `quality_mask` function provided by the EMIT team reads and processes this file for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e6825-5f13-4531-9bb2-d2dcffe0d765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flags=[7]\n",
    "mask = quality_mask(fp_mask,flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099804ec-2f53-4096-b3d3-d08d248b81e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the data\n",
    "As you saw the raw data is the unprojected, unmasked focal plane array and does not include much metadata for us to tell what is going on.  The `emit_array` function provided by the EMIT team reads and processes this file for us. It can orthorectify the data and apply the mask too if we want.\n",
    "\n",
    "> Warning: in order to orthorectify the data, the entire array must be loaded into memory. This means you need a decent amount of RAM or the operation will fail. I found that 16GB is the minimum needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82145bc0-fa23-47fb-9583-dc14ded063bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = emit_xarray(fp, ortho=True,qmask=mask)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114242b3-bd73-4ae7-b37d-03763a6040a1",
   "metadata": {},
   "source": [
    "Now that we have orthorectified the data we can chunk it to make later processes more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fc3a9-4839-4989-baf0-1d8096a1b21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds.chunk({'latitude':-1,'longitude':-1,'wavelengths':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6596a4-d9bf-40e7-8925-8285c4bb0029",
   "metadata": {},
   "source": [
    "The first thing we do is drop the bad bands (those in the atmospheric water absorption zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efff08c-f7dd-41ff-883e-c5ac0e27dd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds.where(ds.good_wavelengths.compute()==1,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426974c-4377-486a-bb71-b94f45cef95a",
   "metadata": {},
   "source": [
    "### Quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2f601-4f1d-41b4-92b9-5bf56c228299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#select rgb\n",
    "rgb = ds.sel(wavelengths=[650, 560, 470], method='nearest')\n",
    "#make colors nicer\n",
    "rgb = gamma_adjust(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d1299-3e46-4573-9f4e-6d2b598ff623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb.hvplot.rgb(x='longitude', y='latitude', bands='wavelengths', aspect = 'equal', frame_width=400,rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124ef90-0cad-4c44-a1c3-b5b72e26d077",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5652860-0680-4bf1-a330-000edc0eb890",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](https://media.tenor.com/adoVIoM4eo4AAAAC/keyboard-cat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484938cb-1269-41f6-9b63-4acc7fd069f8",
   "metadata": {},
   "source": [
    "We will now create synthetically mixed spectra. We start with a spatial data set that contains the labelled locations of pure endmembers for 4 classes: _Fynbos, Pine, Rock and Water_.   \n",
    "First lets extract the spectra at the specified locations from the EMIT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682f4c6-c9b1-470a-a500-1db8cfbc3e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load points\n",
    "gdf = gpd.read_file(\"data/emit_unmix.gpkg\")\n",
    "#add x and y locations as df columns\n",
    "gdf['Longitude'] = gdf.centroid.x\n",
    "gdf['Latitude'] = gdf.centroid.y\n",
    "#drops cols we dont need\n",
    "df = pd.DataFrame(gdf).drop(['geometry','ID'],axis=1)\n",
    "#add id col\n",
    "df['ID'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30440ad0-a44a-4d2a-9c43-88f1a8f5e0c3",
   "metadata": {},
   "source": [
    "We use another function from our utils package `extract_points` to extract the spectra from the emit array and return the results as a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a553f-b410-46d5-b933-440e66a3b1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = extract_points(df,ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d9044-a5b2-4197-b45a-5027325446d3",
   "metadata": {},
   "source": [
    "We need to convert the string names of our endmembers to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4a0c9-91b9-42d9-931f-54a52b6df365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encode labels to int\n",
    "le = LabelEncoder()\n",
    "df['Category'] = le.fit_transform(df['Category'])\n",
    "\n",
    "#we will need this later\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "#convert int64 to int using dict comprehension\n",
    "class_mapping = {k: int(v) if isinstance(v, np.int64) else v for k, v in class_mapping.items()}\n",
    "#write to file\n",
    "json.dump(class_mapping, open('data/classes.json', 'w'))\n",
    "\n",
    "#select only the cols we need\n",
    "df = df[['ID','wavelengths','reflectance','Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce86ef9-4140-44d0-b723-3f3a71f56c85",
   "metadata": {},
   "source": [
    "Convert from long to wide df format and drop rows with na in any column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fe617-24f7-44d6-abf7-9177d89a135f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (df\n",
    "      .pivot(index=['ID','Category'], columns='wavelengths', values='reflectance') #pivot long to wide\n",
    "      .dropna() #drop rows with na\n",
    "      .reset_index() #add index as cols\n",
    "     )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a36276-dc81-4fb5-b6b0-4922d1d4b347",
   "metadata": {},
   "source": [
    "We want to use different endmembers to create the mixtures used to train and test the model. Testing model performance on endmembers it has never seen is a more realisitic simulation of how the model will perform when applied to the real data (an entire emit scene).   \n",
    "  \n",
    "We therefore create a separate endmember library for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322686b-2de8-4088-9fa3-8cfdd127e632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split into train and val\n",
    "df_tr, df_val = train_test_split(df, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7481c5f-e414-4953-aea0-f1ff5aea7296",
   "metadata": {},
   "source": [
    "Now we do the mixing using `mix_fast` from `utils` (it is not very fast), we can specify how many synthetic mixtures we want in the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8177a92-a888-420f-8545-ba2aa42a8d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create synthetic mixtures\n",
    "#takes 5 min if 20000 for each\n",
    "df_tr_mix = mix_fast(df_tr,20000)\n",
    "df_val_mix = mix_fast(df_val,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135d45a-3f2c-4754-b137-325809a4163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for later\n",
    "#df_tr_mix.to_csv('data/trmix.csv', index=False)\n",
    "#df_val_mix.to_csv('data/valmix.csv', index=False)\n",
    "#df_tr_mix = pd.read_csv('data/trmix.csv')\n",
    "#df_val_mix = pd.read_csv('data/valmix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d27eb-bcb6-4991-9ae6-7d64abe12a09",
   "metadata": {},
   "source": [
    "last bit of cleaning involves separating the labels and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea3b36-4e99-4ede-9f72-22d520786ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop cols we dont need\n",
    "df_tr_mix = df_tr_mix.drop(['Category','ID'], axis=1)\n",
    "df_val_mix = df_val_mix.drop(['Category','ID'], axis=1)\n",
    "\n",
    "#index of last wavelength col\n",
    "idx = 243\n",
    "\n",
    "#split into features and labels\n",
    "X_val_mix = df_val_mix.iloc[:, :idx]\n",
    "\n",
    "X_tr_mix = df_tr_mix.iloc[:, :idx]\n",
    "#convert labels to %\n",
    "y_val_mix = df_val_mix.iloc[:, idx:]*100\n",
    "y_tr_mix = df_tr_mix.iloc[:, idx:]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb409c-a07d-45db-acd0-97ed148d5210",
   "metadata": {},
   "source": [
    "Have a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99781f-e97a-459a-b707-1167015d8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first the spectra:\n",
    "X_tr_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37792298-7343-4b95-b442-ad6c59b9cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second the lavbels:\n",
    "y_tr_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eed036-ac3b-4a2f-a7be-73a2aa81e5a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b21c9-2a6a-4d67-887a-6d6e252f25d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### XGBoost\n",
    "We will use XGBoost to fit models. XGBoost is a fast, efficient machine learning library that uses gradient boosting, a method where new models fix errors of prior ones. It's __very__ popular for machine learning on tabular data. It is very fast and usually achieves results superior to Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a86986-8098-4dc3-955e-c600cdb97bb0",
   "metadata": {},
   "source": [
    "We follow a pretty standard python ML workflow\n",
    "- choose a method \n",
    "- define a parameter grid\n",
    "- do a random search over these params for the best model using a 5 fold cross- validation\n",
    "- keep the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e4bc7-b1a9-4000-a25d-218e41e643de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data as np arrays\n",
    "X_tr = np.array(X_tr_mix)\n",
    "X_val = np.array(X_val_mix)\n",
    "y_tr = np.array(y_tr_mix)\n",
    "y_val = np.array(y_val_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4c2be-2442-4b64-a53d-6f9af738f6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb = XGBRegressor(tree_method='hist')\n",
    "\n",
    "# Define parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [5, 10, 20]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "random_search = RandomizedSearchCV(xgb, param_dist, cv=5, n_iter=3)\n",
    "random_search = random_search.fit(X_tr, y_tr)\n",
    "\n",
    "#keep the best model\n",
    "best_model_xgb = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8f3ab-b425-4bed-865b-be8cd22bdd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save the best model\n",
    "#best_model_xgb.save_model('models/best_xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ea3d7-6781-40bf-b7a8-6cd3d8d1d419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reload the best model\n",
    "#best_model_xgb = XGBRegressor()\n",
    "#best_model_xgb.load_model('models/best_xgb_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1c033-d789-4d66-8839-ca294381d272",
   "metadata": {},
   "source": [
    "#### Inspect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58282166-4e54-49d2-8cb1-64ac84d0e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation data\n",
    "val_pred = best_model_xgb.predict(X_val)\n",
    "val_pred = np.clip(val_pred,0,100)\n",
    "\n",
    "# Predict on training data\n",
    "tr_pred = best_model_xgb.predict(X_tr)\n",
    "tr_pred = np.clip(tr_pred,0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7ab27-58c7-41be-8abd-e7c5526b7f09",
   "metadata": {},
   "source": [
    "#### Plot results for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b4011-381e-4242-88ae-667609847600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training samples\n",
    "nsamples = tr_pred.shape[0]\n",
    "noutputs = tr_pred.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(noutputs, figsize=(10, 4*noutputs))\n",
    "\n",
    "for i in range(noutputs):\n",
    "    rmse = np.sqrt(mean_squared_error(y_tr[:, i], tr_pred[:, i]))\n",
    "    key = list(class_mapping.keys())[list(class_mapping.values()).index(i)]\n",
    "    axs[i].scatter(y_tr[:, i],tr_pred[:, i],alpha=0.2)\n",
    "    axs[i].set_title(f'Training class: {key}, RMSE: {rmse:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902ff72-697d-455b-8983-3df83fb04a05",
   "metadata": {},
   "source": [
    "#### Plot results for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7ed13-cc57-4616-a08a-926403399050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation samples\n",
    "nsamples = val_pred.shape[0]\n",
    "noutputs = val_pred.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(noutputs, figsize=(10, 4*noutputs))\n",
    "\n",
    "for i in range(noutputs):\n",
    "    rmse = np.sqrt(mean_squared_error(y_val[:, i], val_pred[:, i]))\n",
    "    key = list(class_mapping.keys())[list(class_mapping.values()).index(i)]\n",
    "    axs[i].scatter(y_val[:, i],val_pred[:, i],alpha=0.2)\n",
    "    axs[i].set_title(f'Validation class: {key}, RMSE: {rmse:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347634c-ae88-433b-a78e-fa434b62ca24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bonus: another ML method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c13c1c-ebd3-418c-b573-b159954b71bc",
   "metadata": {},
   "source": [
    "[ROCKET](https://github.com/angus924/rocket) is a method for time series classification, making it specifically suitable for 1D data with temporal (or spectral) ordering. It works by generating a large number of random convolutional kernels which are applied to an input time series to produce features which are used with a linear classifier. It may be a better choice for this data as XGBoost is a method designed for tabluar data - it does not account for the ordering of features and thus is agnostic to the spectral postion. You can try to fit rocket models below, but beware, they can take longer than xgboost models to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb51fd2-94ff-442c-b28d-8a49da3361c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sktime.regression.kernel_based import RocketRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721e336-74c4-41b4-87fc-47f23d719ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#params to search over\n",
    "param_dist = {\n",
    "    'estimator__num_kernels': [100, 500, 2000], \n",
    "}\n",
    "\n",
    "# Define the estimator\n",
    "reg = MultiOutputRegressor(RocketRegressor(rocket_transform='minirocket'))\n",
    "\n",
    "# Perform the random search\n",
    "random_search = RandomizedSearchCV(reg, param_dist, cv=5, n_iter=3)\n",
    "random_search.fit(X_tr,y_tr)\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4285879-8a0c-4f2b-9629-7fb9463376f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('models/rocketmodel.pkl', 'wb') as f:\n",
    "#    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2426415-1b10-4f54-86db-2e1f413de83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('models/rocketmodel.pkl', 'rb') as f:\n",
    "#    best_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c88625-689f-40b5-a8f4-b1c46a4273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation data\n",
    "val_pred = best_model.predict(X_val)\n",
    "val_pred = np.clip(val_pred,0,100)\n",
    "\n",
    "# Predict on training data\n",
    "tr_pred = best_model.predict(X_tr)\n",
    "tr_pred = np.clip(tr_pred,0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bc389-9b15-454c-8348-50cea310f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training samples\n",
    "nsamples = tr_pred.shape[0]\n",
    "noutputs = tr_pred.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(noutputs, figsize=(10, 4*noutputs))\n",
    "\n",
    "for i in range(noutputs):\n",
    "    rmse = np.sqrt(mean_squared_error(y_tr[:, i], tr_pred[:, i]))\n",
    "    key = list(class_mapping.keys())[list(class_mapping.values()).index(i)]\n",
    "    axs[i].scatter(y_tr[:, i],tr_pred[:, i],alpha=0.2)\n",
    "    axs[i].set_title(f'Training class: {key}, RMSE: {rmse:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28931b5b-29e4-4356-9517-7fa806b758a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation samples\n",
    "nsamples = val_pred.shape[0]\n",
    "noutputs = val_pred.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(noutputs, figsize=(10, 4*noutputs))\n",
    "\n",
    "for i in range(noutputs):\n",
    "    rmse = np.sqrt(mean_squared_error(y_val[:, i], val_pred[:, i]))\n",
    "    key = list(class_mapping.keys())[list(class_mapping.values()).index(i)]\n",
    "    axs[i].scatter(y_val[:, i],val_pred[:, i],alpha=0.2)\n",
    "    axs[i].set_title(f'Validation class: {key}, RMSE: {rmse:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c12525-e658-4c72-a4c6-eb8d3177b7fc",
   "metadata": {},
   "source": [
    "## credits:\n",
    "\n",
    "This lesson has borrowed from:    \n",
    "\n",
    "[the EMIT-Data-Resources repository by LPDAAC/ the EMIT team](https://github.com/nasa/EMIT-Data-Resources) \n",
    "\n",
    "[Okunjeni et al 2013 RSE for the original methodology](https://www.sciencedirect.com/science/article/abs/pii/S0034425713002009)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:emitpy]",
   "language": "python",
   "name": "conda-env-emitpy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
